{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac897d1a",
   "metadata": {},
   "source": [
    "### Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35e6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import optparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sumolib import checkBinary  \n",
    "import traci \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fd7b4",
   "metadata": {},
   "source": [
    "### Defining three functions:\n",
    "#### 1) tally_vehicles_in_lanes: to count the number of vehciles in each lane in the network used for simulation\n",
    "#### 2) tally_vehicles_in_lanes: to calculate the total waiting time of vehicles in all lanes\n",
    "#### 3) configure_trafficlight_phase: to configure the traffic light phase at a junction with a certain duration and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfa0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_vehicles_in_lanes(road_lanes):\n",
    "    lane_vehicle_count = {}\n",
    "    for lane_id in road_lanes:\n",
    "        lane_vehicle_count[lane_id] = sum(\n",
    "            1 for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) \n",
    "            #traci.lane.getLastStepVehicleIDs is a method from TraCI API.\n",
    "            \n",
    "            if traci.vehicle.getLanePosition(veh_id) > 10\n",
    "        )\n",
    "    return lane_vehicle_count\n",
    "    \n",
    "    \"\"\"\n",
    "    this method first takes a list of all lane ids as the argument in order to calculate the total number of vehicles\n",
    "    present in each lane provided by the getLastStepVehicleIDs.\n",
    "    It then checks if the vehciles which are present in the network are within the certain lenght of the lane or not \n",
    "    (in this case 10 meters of the lane). The purpose of this check is likely to exclude vehicles that are still in the \n",
    "    process of entering the lane and have not fully entered the lane yet. \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef7a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_waiting_time(road_lanes):\n",
    "    return sum(traci.lane.getWaitingTime(single_lane) for single_lane in road_lanes)\n",
    "\n",
    "    \"\"\"\n",
    "    compute_total_waiting_time takes a list of lane ids (road_lanes) as input and returns the sum of \n",
    "    waiting times for each lane.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5adb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_trafficlight_phase(junction_id, phase_duration, phase_config):\n",
    "    traci.trafficlight.setRedYellowGreenState(junction_id, phase_config)\n",
    "    traci.trafficlight.setPhaseDuration(junction_id, phase_duration)\n",
    "    \n",
    "    \"\"\"\n",
    "    -> This function is used to configue the trafffic light at a particular junction for a specific amount of time.\n",
    "    -> setRedYellowGreenState(junction_id, phase_config)--> takes two arguments as we can see. It gets the id for each \n",
    "       intersection and then set the traffic phase based on \"phase_config\"\n",
    "    -> setPhaseDuration(junction_id, phase_duration)--> it also takes two arguments which also take id of intersection\n",
    "        and set the traffic phase for a certain amount of time.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33170cc",
   "metadata": {},
   "source": [
    "### Defining Deep Neural Network model using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2eb8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module): ## inheriting from nn.Module class provided by Pytorch\n",
    "    def __init__(self, learning_rate, input_dimensions, layer1_dimensions, layer2_dimensions, action_count):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.input_dimensions = input_dimensions \n",
    "        #defining the number of input features for the network\n",
    "        \n",
    "        self.layer1_dimensions = layer1_dimensions\n",
    "        #defining the number of nodes in the first hideen layer\n",
    "        \n",
    "        self.layer2_dimensions = layer2_dimensions \n",
    "        #definign the nimber of nodes in the second hidden layer\n",
    "        \n",
    "        self.action_count = action_count \n",
    "        #defining the number of possible actions the network can output\n",
    "\n",
    "        self.layer1 = nn.Linear(self.input_dimensions, self.layer1_dimensions)\n",
    "        #creating the first linear transformation layer\n",
    "        \n",
    "        self.layer2 = nn.Linear(self.layer1_dimensions, self.layer2_dimensions)\n",
    "        #creating the second linear transformation layer\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.layer2_dimensions, self.action_count)\n",
    "        #creating the third linear transformation layer which is also the final and output layer\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # we are using Adam Optimizer in our program since it update the learning rate for each parameter \n",
    "        # based on the history of gradient and has good convergenbce speed\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # We are using Mean Squared Error (MSE) loss function for our model. Computing the difference between \n",
    "        # the predicted output of the network and the target model\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "\n",
    "    def forward(self, input_state):\n",
    "        #after already defining the input layer, hidden layers and output layer, now defining the output function for \n",
    "        # each layer defined earlier\n",
    "        \n",
    "        layer1_output = F.relu(self.layer1(input_state))\n",
    "        # layer1 output will activate the non-linear activation funciton which is ReLU in our case and  it will be passed\n",
    "        # to another layer for processing which will again be converted to Linear tarnsformation\n",
    "        \n",
    "        layer2_output = F.relu(self.layer2(layer1_output))\n",
    "        # same like in layer1 ouput, it will again be converted to non-linear transformation and then passed to another \n",
    "        # layer for further processing\n",
    "        \n",
    "        action_values = self.output_layer(layer2_output)\n",
    "        # in this case, we won'tbe activating the non-linear transformation because this is the output layer.\n",
    "        return action_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ba216",
   "metadata": {},
   "source": [
    "### Defining Agent for our reinforcement learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d093e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating RLAgent class which represents a reinforcement learning agent that interacts with the environment, stores \n",
    "and learns from experience in memory, and selects actions based on a trained neural network.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class RLAgent:\n",
    "    # Constructor method to initialize the RLAgent class with necessary parameters\n",
    "    def __init__(\n",
    "        self,\n",
    "        discount_factor,\n",
    "        exploration_rate,\n",
    "        learning_rate,\n",
    "        input_dimensions,\n",
    "        layer1_dimensions,\n",
    "        layer2_dimensions,\n",
    "        batch_size,\n",
    "        action_count,\n",
    "        junctions,\n",
    "        max_memory_size=100000,\n",
    "        \n",
    "        # max_memory_size is set to 100000, \n",
    "        # which means that each junction's memory buffer can store up to 99999 experience tuples.\n",
    "        \n",
    "        exploration_rate_decay=0.0005,\n",
    "        # exploration_rate_decay is set to 0.0005, which means that the exploration rate will \n",
    "        # be reduced by a factor of 0.0005 after each iteration\n",
    "        \n",
    "        exploration_rate_end=0.05,\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialize the RLAgent instance variables with the given parameters\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.layer1_dimensions = layer1_dimensions\n",
    "        self.layer2_dimensions = layer2_dimensions\n",
    "        self.action_count = action_count\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create a list of actions based on the action count\n",
    "        self.action_space = [i for i in range(action_count)]\n",
    "        \n",
    "        # Set the junctions\n",
    "        self.junctions = junctions\n",
    "        \n",
    "        # Set the maximum memory size for the RLAgent\n",
    "        self.max_memory = max_memory_size\n",
    "        \n",
    "        # Set the exploration rate decay\n",
    "        self.exploration_rate_decay = exploration_rate_decay\n",
    "        \n",
    "        # Set the end value for the exploration rate\n",
    "        self.exploration_rate_end = exploration_rate_end\n",
    "        \n",
    "        # Initialize memory counter and iteration counter\n",
    "        self.memory_counter = 0\n",
    "        self.iteration_counter = 0\n",
    "        \n",
    "        \n",
    "        # Set the target replace\n",
    "        self.target_replace = 100\n",
    "        \n",
    "        # Create a neural network instance to evaluate the action values\n",
    "        self.eval_net = NeuralNetwork(\n",
    "            self.learning_rate, self.input_dimensions, self.layer1_dimensions, self.layer2_dimensions, self.action_count\n",
    "        )\n",
    "        \n",
    "        # Create a memory dictionary for each junction with state memory, new state memory, reward memory, \n",
    "        # action memory, terminal memory, memory counter and iteration counter\n",
    "        self.memory = dict()\n",
    "        for junction in junctions:\n",
    "            self.memory[junction] = {\n",
    "                \"state_memory\": np.zeros(\n",
    "                    (self.max_memory, self.input_dimensions), dtype=np.float32\n",
    "                ),\n",
    "                \"new_state_memory\": np.zeros(\n",
    "                    (self.max_memory, self.input_dimensions), dtype=np.float32\n",
    "                ),\n",
    "                \"reward_memory\": np.zeros(self.max_memory, dtype=np.float32),\n",
    "                \"action_memory\": np.zeros(self.max_memory, dtype=np.int32),\n",
    "                \"terminal_memory\": np.zeros(self.max_memory, dtype=bool),\n",
    "                \"memory_counter\": 0,\n",
    "                \"iteration_counter\": 0,\n",
    "            }\n",
    "            \n",
    "    # Store the state, new state, action, reward, done and junction information in memory\n",
    "    def store_transition(self, state, new_state, action, reward, done, junction):\n",
    "        \n",
    "        index = self.memory[junction][\"memory_counter\"] % self.max_memory\n",
    "        # it is important in experience replay to keep the memory store updated. Since, our memory tuple capactiy is of \n",
    "        # size 100000, we need to keep the memory updated on regular basis. Thus, we are checking the index of current\n",
    "        # memory in memory store and using % to find the index. \n",
    "        # How does that work?\n",
    "        # The % operator is the modulo operator, which returns the remainder of a division operation. \n",
    "        # In this case, it is used to calculate the index at which the new experience tuple should be stored in the \n",
    "        # memory buffer.\n",
    "        \n",
    "        self.memory[junction][\"state_memory\"][index] = state\n",
    "        self.memory[junction][\"new_state_memory\"][index] = new_state\n",
    "        self.memory[junction]['reward_memory'][index] = reward\n",
    "        self.memory[junction]['terminal_memory'][index] = done\n",
    "        self.memory[junction][\"action_memory\"][index] = action\n",
    "        self.memory[junction][\"memory_counter\"] += 1\n",
    "\n",
    "        \n",
    "        \n",
    "    # Select an action based on the current observation and exploration rate    \n",
    "    def select_action(self, observation):\n",
    "        state = torch.tensor([observation], dtype=torch.float).to(self.eval_net.device)\n",
    "        # passing the current state of the environment as input to the state variable which will create the multi-\n",
    "        # dimensional array and then move it to neural network model in the same work environment.\n",
    "        \n",
    "        \n",
    "        # Decide whether to explore or exploit based on the exploration rate\n",
    "        if np.random.random() > self.exploration_rate:\n",
    "            actions = self.eval_net.forward(state)\n",
    "            action = torch.argmax(actions).item()\n",
    "            \n",
    "            \n",
    "        # Explore by selecting a random action from the action space    \n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    \n",
    "    # Reset memory for the specified junctions\n",
    "    def reset(self, junction_numbers):\n",
    "        for junction_number in junction_numbers:\n",
    "            self.memory[junction_number]['memory_counter'] = 0\n",
    "            \n",
    "            \"\"\"\n",
    "            Since we are using this agent in an environment where there are multiple episodes or trials, we have to \n",
    "            to reset the memory counters for specific junctions at the beginning of each new episode or trial. \n",
    "            This is done to ensure that the agent starts with a clean slate in terms of memory for the new episode or \n",
    "            trial.\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "    # Save the trained model to a file        \n",
    "    def save(self, model_name):\n",
    "        torch.save(self.eval_net.state_dict(), f'trained_model/{model_name}.bin')\n",
    "        \n",
    "        \"\"\"\n",
    "        We are saving the model for further uses later like in case of demonstration of simulation. \n",
    "        We can use the same model to see how our model performed by loading it in SUMO simulator.\n",
    "        Moreover, we can also use it for other things like checkpointing, model evaluation, transfer learning, sharing\n",
    "            and collaboration and so on.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "    \n",
    "    # Update the evaluation network with the stored transitions    \n",
    "    \"\"\"\n",
    "    After the forward feed is done in our neural network mode, we will then have to look at our memory and make some\n",
    "    improvements in our neural network parameters. We take the current intersection and then based on that intersection's \n",
    "    memory.\n",
    "    \n",
    "    \"\"\"\n",
    "    def learn(self, junction):\n",
    "        \n",
    "        \"\"\"\n",
    "         This method updates the agent's neural network based on the agent's experiences stored in memory. \n",
    "         It calculates the loss between the Q-values estimated by the neural network and the target Q-values, \n",
    "         then uses backpropagation and an optimizer to update the neural network parameters. The method also updates \n",
    "         the agent's iteration counter and exploration rate, allowing the agent to balance exploration and exploitation \n",
    "         during the learning process.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Zero the gradient of the optimizer\n",
    "        self.eval_net.optimizer.zero_grad() #to avoid accumulating gradients from previous learning steps\n",
    "\n",
    "        # batch of memory samples: creating an array of integers from 0 to the memory counter for the given junction\n",
    "        batch = np.arange(self.memory[junction]['memory_counter'], dtype=np.int32) \n",
    "\n",
    "        # Create tensors for the state, new state, reward, terminal and action\n",
    "        state_batch = torch.tensor(self.memory[junction][\"state_memory\"][batch]).to(\n",
    "            self.eval_net.device\n",
    "        )\n",
    "        new_state_batch = torch.tensor(\n",
    "            self.memory[junction][\"new_state_memory\"][batch]\n",
    "        ).to(self.eval_net.device)\n",
    "        reward_batch = torch.tensor(\n",
    "            self.memory[junction]['reward_memory'][batch]).to(self.eval_net.device)\n",
    "        terminal_batch = torch.tensor(self.memory[junction]['terminal_memory'][batch]).to(self.eval_net.device)\n",
    "        action_batch = self.memory[junction][\"action_memory\"][batch]\n",
    "\n",
    "        \n",
    "        # Calculate the expected value of the action based on the evaluation network\n",
    "        q_eval = self.eval_net.forward(state_batch)[batch, action_batch]\n",
    "    \n",
    "        # Calculate the expected value of the next state\n",
    "        q_next = self.eval_net.forward(new_state_batch)\n",
    "        \n",
    "        # Set the value of the terminal state to zero\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        # Calculate the target value for the action\n",
    "        q_target = reward_batch + self.discount_factor * torch.max(q_next, dim=1)[0]\n",
    "        \n",
    "        # Calculate the loss function based on the evaluation network and target value\n",
    "        loss_function = self.eval_net.loss_function(q_target, q_eval).to(self.eval_net.device)\n",
    "\n",
    "        # Backpropagate the error through the evaluation network and update the parameters using the optimizer\n",
    "        loss_function.backward()\n",
    "        self.eval_net.optimizer.step()\n",
    "        \n",
    "        # Increment the iteration counter and update the exploration rate\n",
    "        self.iteration_counter += 1\n",
    "        self.exploration_rate = (\n",
    "            self.exploration_rate - self.exploration_rate_decay\n",
    "            if self.exploration_rate > self.exploration_rate_end\n",
    "            else self.exploration_rate_end\n",
    "        )\n",
    "        \n",
    "    def get_exploration_rate(self):\n",
    "        return self.exploration_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872fc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6a44e1f",
   "metadata": {},
   "source": [
    "### Define a function to run the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5d6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(train=True, model_name=\"model\", epochs=200, steps=500):\n",
    "    \n",
    "    # Set the number of epochs and steps\n",
    "    epochs = epochs\n",
    "    steps = steps\n",
    "    \n",
    "    # Set the best time as infinity such that at first the best time is set to infinity and after second run, the best time\n",
    "    # will be set to a fixed number and comparision for further best time would be easier\n",
    "    best_time = np.inf\n",
    "    \n",
    "    # Create an empty list to store the total time\n",
    "    total_time_list = list()\n",
    "    \n",
    "    exploration_rate_list = list()\n",
    "    cumulative_reward_list = []\n",
    "    epsilon_list = []\n",
    "    \n",
    "    # Start the TraCI simulation with sumo and configuration file with tripinfo output\n",
    "    traci.start(\n",
    "        [checkBinary(\"sumo\"), \"-c\", \"configuration.sumocfg\", \"--tripinfo-output\", \"network/tripinfo.xml\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Get the list of all junctions\n",
    "    all_junctions = traci.trafficlight.getIDList()\n",
    "    \n",
    "    # Create a list of junction numbers\n",
    "    junction_numbers = list(range(len(all_junctions)))\n",
    "\n",
    "    # Initialize the RLAgent with necessary parameters\n",
    "    rl_agent = RLAgent(\n",
    "        discount_factor=0.99,\n",
    "        exploration_rate=0.0,\n",
    "        learning_rate=0.1,\n",
    "        input_dimensions=4,\n",
    "        layer1_dimensions=256,\n",
    "        layer2_dimensions=256,\n",
    "        batch_size=1024,\n",
    "        action_count=4,\n",
    "        junctions=junction_numbers,\n",
    "    )\n",
    "\n",
    "    # Load the trained model weights if not training\n",
    "    \"\"\"\n",
    "    When we enter the commnad or when we execute the file to run the program, the program checks for train validation, \n",
    "    if we have set the train boolean value to true then the model will start training else, it will look for the last\n",
    "    trained model and open it in SUMO GUI software to look at the simulation.\n",
    "    \"\"\"\n",
    "    if not train:\n",
    "        rl_agent.eval_net.load_state_dict(torch.load(f'trained_model/{model_name}.bin', map_location=rl_agent.eval_net.device))\n",
    "\n",
    "    \n",
    "    # Close the TraCI simulation\n",
    "    traci.close()\n",
    "    \n",
    "    accumulated_rewards = list()\n",
    "\n",
    "    average_waiting_time_list = []\n",
    "    \n",
    "    # Run the simulation for the specified number of epochs\n",
    "    for e in range(epochs):\n",
    "        total_vehicles = 0\n",
    "        # Start the TraCI simulation with sumo and configuration file with tripinfo output\n",
    "        if train:\n",
    "            traci.start(\n",
    "            [checkBinary(\"sumo\"), \"-c\", \"configuration.sumocfg\", \"--tripinfo-output\", \"tripinfo.xml\"]\n",
    "            )\n",
    "        else:\n",
    "            traci.start(\n",
    "            [checkBinary(\"sumo-gui\"), \"-c\", \"configuration.sumocfg\", \"--tripinfo-output\", \"tripinfo.xml\"]\n",
    "            )\n",
    "\n",
    "        # Print the epoch number\n",
    "        print(f\"Simulating episode number: {e}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        select_lane = [\n",
    "            [\"yyyyrrrrrrrrrrrr\", \"GGGGrrrrrrrrrrrr\"],\n",
    "            [\"rrrryyyyrrrrrrrr\", \"rrrrGGGGrrrrrrrr\"],\n",
    "            [\"rrrrrrrryyyyrrrr\", \"rrrrrrrrGGGGrrrr\"],\n",
    "            [\"rrrrrrrrrrrryyyy\", \"rrrrrrrrrrrrGGGG\"],\n",
    "        ]\n",
    "        \"\"\"\n",
    "        This is one of the very important section of the code. This a list containing four elements, each representing \n",
    "        the possible lane configurations for the traffic lights at the junctions. Each element consists of two strings, \n",
    "        the first string indicates the red (waiting) and yellow (transition) light configuration, while the second string \n",
    "        indicates the green (moving) light configuration. The purpose of this list is to provide a mapping for the RLAgent \n",
    "        to select the appropriate action for a given state.\n",
    "        \n",
    "        The traffic light pattern can be manually created or we can use the logic behind each traffic light which is given\n",
    "        in the .net.xml file which we can retrieve for a city from OpenStreetMap.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        step = 0\n",
    "        total_time = 0\n",
    "        min_duration = 5\n",
    "        cumulative_reward = 0\n",
    "        epsilon_list.append(rl_agent.exploration_rate)\n",
    "        \n",
    "        # Create dictionaries to store previous wait time, action, and vehicles per lane\n",
    "        traffic_lights_time = dict()\n",
    "        prev_wait_time = dict()\n",
    "        prev_vehicles_per_lane = dict()\n",
    "        prev_action = dict()\n",
    "        all_lanes = list()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        These variables are used during the simulation to track previous waiting times, previous actions taken, \n",
    "        traffic light timings, and the number of vehicles in each lane for each junction. The loop iterates through each \n",
    "        junction in the all_junctions list.\n",
    "        \"\"\"\n",
    "        # For each junction initialize the dictionaries\n",
    "        for junction_number, junction in enumerate(all_junctions):\n",
    "            prev_wait_time[junction] = 0\n",
    "            prev_action[junction_number] = 0\n",
    "            traffic_lights_time[junction] = 0\n",
    "            prev_vehicles_per_lane[junction_number] = [0] * 4\n",
    "            # prev_vehicles_per_lane[junction_number] = [0] * (len(all_junctions) * 4) \n",
    "            all_lanes.extend(list(traci.trafficlight.getControlledLanes(junction)))\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        while step <= steps:\n",
    "            # Advance the simulation by one step\n",
    "            traci.simulationStep()\n",
    "            exploration_rate_sum = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            # For each junction compute total waiting time and add it to the total time\n",
    "            for junction_number, junction in enumerate(all_junctions):\n",
    "                controled_lanes = traci.trafficlight.getControlledLanes(junction)\n",
    "                waiting_time = compute_total_waiting_time(controled_lanes)\n",
    "                total_time += waiting_time\n",
    "                accumulated_reward = sum([traci.lane.getWaitingTime(single_lane) for single_lane in all_lanes])\n",
    "                \n",
    "                num_vehicles = traci.vehicle.getIDCount()\n",
    "                total_vehicles += num_vehicles\n",
    "                \n",
    "                \"\"\"\n",
    "                This is important for calculating the total waiting time and reward for each junction and for the \n",
    "                entire simulation. These values are used to evaluate the performance of the reinforcement learning agent \n",
    "                and to optimize the traffic light patterns at each junction to minimize waiting time and improve traffic \n",
    "                flow.\n",
    "                \"\"\"\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # If traffic light phase is not active, then get the vehicles per lane and select new action\n",
    "                if traffic_lights_time[junction] == 0:\n",
    "                    vehicles_per_lane = tally_vehicles_in_lanes(controled_lanes)\n",
    "                    # vehicles_per_lane = get_vehicle_numbers(all_lanes)\n",
    "\n",
    "                    # Store the previous and current state and the corresponding reward\n",
    "                    reward = -1 *  waiting_time\n",
    "                    state_ = list(vehicles_per_lane.values()) \n",
    "                    state = prev_vehicles_per_lane[junction_number]\n",
    "                    cumulative_reward += reward \n",
    "                    prev_vehicles_per_lane[junction_number] = state_\n",
    "                    rl_agent.store_transition(state, state_, prev_action[junction_number],reward,(step==steps),junction_number)\n",
    "\n",
    "                    #selecting new action based on current state\n",
    "                    lane = rl_agent.select_action(state_)\n",
    "                    prev_action[junction_number] = lane\n",
    "                    configure_trafficlight_phase(junction, 6, select_lane[lane][0])\n",
    "                    configure_trafficlight_phase(junction, min_duration + 10, select_lane[lane][1])\n",
    "                    \"\"\"\n",
    "                     is responsible for selecting a new action for each traffic light at a junction based on the current \n",
    "                     state and updating the simulation environment with the new traffic light phase pattern. This is \n",
    "                     crucial for optimizing traffic flow and minimizing waiting time.\n",
    "                    \"\"\"\n",
    "\n",
    "                    \n",
    "\n",
    "                    # Set the traffic light time and learn the model for the junction\n",
    "                    traffic_lights_time[junction] = min_duration + 10\n",
    "                    if train:\n",
    "                        rl_agent.learn(junction_number)\n",
    "                        exploration_rate_sum += rl_agent.get_exploration_rate()\n",
    "                        \n",
    "                # If traffic light phase is active, then decrease the time left\n",
    "                else:\n",
    "                    traffic_lights_time[junction] -= 1\n",
    "                    \n",
    "            # Increase the step counter        \n",
    "            step += 1\n",
    "          \n",
    "        \n",
    "        # Print the total waiting time for each car at each junction\n",
    "        print(\"Total time every cars waited at junction for: \",total_time)\n",
    "        \n",
    "        avg_exploration_rate = exploration_rate_sum / steps\n",
    "        exploration_rate_list.append(avg_exploration_rate)\n",
    "        \n",
    "        average_waiting_time_per_vehicle = total_time / total_vehicles if total_vehicles > 0 else 0\n",
    "        average_waiting_time_list.append(average_waiting_time_per_vehicle)\n",
    "        \n",
    "        total_time_list.append(total_time)\n",
    "        \n",
    "        accumulated_rewards.append(accumulated_reward)\n",
    "\n",
    "        # If the total time is the best so far, save the model\n",
    "        if total_time < best_time:\n",
    "            best_time = total_time\n",
    "            if train:\n",
    "                rl_agent.save(model_name)\n",
    "\n",
    "       \n",
    "        # Close the TraCI simulation\n",
    "        cumulative_reward_list.append(cumulative_reward)\n",
    "        traci.close()\n",
    "        sys.stdout.flush()\n",
    "        if not train:\n",
    "            break\n",
    "            \n",
    "    # If training, plot the waiting time of cars per lane for each epoch        \n",
    "    if train:\n",
    "        plt.title(\"Waiting time of cars per lane vs Episodes\")\n",
    "        plt.plot(list(range(len(total_time_list))),total_time_list, linestyle='--', color='red', marker='o')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Vehicles Waiting Time\")\n",
    "        plt.savefig(f'result/time_vs_epoch_{model_name}.png')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title(\"Average Waiting Time per Vehicle vs Epochs\")\n",
    "        plt.plot(list(range(len(average_waiting_time_list))), average_waiting_time_list, linestyle='--', color='blue', marker='o')\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Average waiting time per vehicle\")\n",
    "        plt.savefig(f'result/average_waiting_time_vs_episode_{model_name}.png')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(\"Accumulated Reward vs Episodes\")\n",
    "        plt.plot(list(range(len(cumulative_reward_list))), cumulative_reward_list, linestyle='-', color='green', marker='o')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Accumulated Reward\")\n",
    "        plt.savefig(f'result/cumulative_reward_per_epoch_{model_name}.png')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.title(\"Epsilon Value over Time\")\n",
    "#         plt.plot(list(range(len(epsilon_list))), epsilon_list, linestyle='-', color='blue', marker='o')\n",
    "#         plt.xlabel(\"Epochs\")\n",
    "#         plt.ylabel(\"Epsilon Value\")\n",
    "#         plt.savefig(f'result/epsilon_value_over_time_{model_name}.png')\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13ea9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating epoch number: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Run the simulation with the given parameters\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 146\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(train, model_name, epochs, steps)\u001b[0m\n\u001b[0;32m    144\u001b[0m waiting_time \u001b[38;5;241m=\u001b[39m compute_total_waiting_time(controled_lanes)\n\u001b[0;32m    145\u001b[0m total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m waiting_time\n\u001b[1;32m--> 146\u001b[0m accumulated_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([traci\u001b[38;5;241m.\u001b[39mlane\u001b[38;5;241m.\u001b[39mgetWaitingTime(single_lane) \u001b[38;5;28;01mfor\u001b[39;00m single_lane \u001b[38;5;129;01min\u001b[39;00m all_lanes])\n\u001b[0;32m    148\u001b[0m num_vehicles \u001b[38;5;241m=\u001b[39m traci\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetIDCount()\n\u001b[0;32m    149\u001b[0m total_vehicles \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_vehicles\n",
      "Cell \u001b[1;32mIn[7], line 146\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m waiting_time \u001b[38;5;241m=\u001b[39m compute_total_waiting_time(controled_lanes)\n\u001b[0;32m    145\u001b[0m total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m waiting_time\n\u001b[1;32m--> 146\u001b[0m accumulated_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetWaitingTime\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_lane\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m single_lane \u001b[38;5;129;01min\u001b[39;00m all_lanes])\n\u001b[0;32m    148\u001b[0m num_vehicles \u001b[38;5;241m=\u001b[39m traci\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetIDCount()\n\u001b[0;32m    149\u001b[0m total_vehicles \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_vehicles\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_lane.py:240\u001b[0m, in \u001b[0;36mLaneDomain.getWaitingTime\u001b[1;34m(self, laneID)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetWaitingTime\u001b[39m(\u001b[38;5;28mself\u001b[39m, laneID):\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;124;03m\"\"\"getWaitingTime() -> double\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    .\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAR_WAITING_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaneID\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    154\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:225\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:128\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recvExact()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is the main entry point of this script\n",
    "if __name__ == \"__main__\":\n",
    "    # Assign the model name, training flag, number of epochs, and number of steps\n",
    "    model_name = 'model'\n",
    "    training = True\n",
    "    num_epochs = 200\n",
    "    num_steps = 500\n",
    "    \n",
    "    # Run the simulation with the given parameters\n",
    "    run_simulation(train=training, model_name=model_name, epochs=num_epochs, steps=num_steps)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
